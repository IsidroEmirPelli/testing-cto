#!/usr/bin/env python3
"""
Script de prueba para todos los scrapers.
Demuestra que el sistema soporta m√∫ltiples scrapers independientes.
"""
import os
import sys
import logging
import asyncio
import django

# Configurar el entorno de Django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'src.infrastructure.config.django_settings')
django.setup()

from asgiref.sync import sync_to_async

from src.infrastructure.adapters.scrapers.clarin_scraper import ClarinScraper
from src.infrastructure.adapters.scrapers.pagina12_scraper import Pagina12Scraper
from src.infrastructure.adapters.scrapers.lanacion_scraper import LaNacionScraper
from src.infrastructure.persistence.django_repositories import DjangoNewsArticleRepository
from src.application.use_cases.scrape_and_persist_articles import ScrapeAndPersistArticlesUseCase
from src.infrastructure.persistence.django_app.models import NewsArticleModel

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(name)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

logger = logging.getLogger(__name__)


async def run_scraper(scraper_name: str, scraper_class, repository):
    """Ejecuta un scraper y retorna los resultados."""
    logger.info(f"\n{'='*80}")
    logger.info(f"EJECUTANDO SCRAPER: {scraper_name}")
    logger.info(f"{'='*80}")
    
    try:
        scraper = scraper_class(max_articles=10)
        use_case = ScrapeAndPersistArticlesUseCase(scraper, repository)
        result = await use_case.execute()
        
        logger.info(f"\n‚úÖ {scraper_name} - Resultados:")
        logger.info(f"   Total scrapeado: {result['total_scraped']}")
        logger.info(f"   Nuevos: {result['total_new']}")
        logger.info(f"   Duplicados: {result['total_duplicates']}")
        
        return result
    except Exception as e:
        logger.error(f"\n‚ùå Error en {scraper_name}: {e}", exc_info=True)
        return None


async def main():
    """Funci√≥n principal para ejecutar todos los scrapers."""
    logger.info("="*80)
    logger.info("TEST DE TODOS LOS SCRAPERS")
    logger.info("="*80)
    logger.info("Prop√≥sito: Verificar que el sistema soporta m√∫ltiples scrapers independientes")
    logger.info("="*80)
    
    # Contar art√≠culos iniciales
    logger.info("\nüìä ESTADO INICIAL DE LA BASE DE DATOS:")
    try:
        count_clarin = await sync_to_async(NewsArticleModel.objects.filter(fuente="Clar√≠n").count)()
        count_pagina12 = await sync_to_async(NewsArticleModel.objects.filter(fuente="P√°gina 12").count)()
        count_lanacion = await sync_to_async(NewsArticleModel.objects.filter(fuente="La Naci√≥n").count)()
        total = await sync_to_async(NewsArticleModel.objects.count)()
        
        logger.info(f"   Clar√≠n: {count_clarin} art√≠culos")
        logger.info(f"   P√°gina 12: {count_pagina12} art√≠culos")
        logger.info(f"   La Naci√≥n: {count_lanacion} art√≠culos")
        logger.info(f"   TOTAL: {total} art√≠culos")
    except Exception as e:
        logger.error(f"Error al consultar la base de datos: {e}")
        return 1
    
    # Inicializar repositorio
    repository = DjangoNewsArticleRepository()
    
    # Ejecutar scrapers
    results = {}
    
    # Scraper 1: Clar√≠n
    result_clarin = await run_scraper("Clar√≠n", ClarinScraper, repository)
    if result_clarin:
        results["Clar√≠n"] = result_clarin
    
    # Scraper 2: P√°gina 12
    result_pagina12 = await run_scraper("P√°gina 12", Pagina12Scraper, repository)
    if result_pagina12:
        results["P√°gina 12"] = result_pagina12
    
    # Scraper 3: La Naci√≥n
    result_lanacion = await run_scraper("La Naci√≥n", LaNacionScraper, repository)
    if result_lanacion:
        results["La Naci√≥n"] = result_lanacion
    
    # Resumen final
    logger.info("\n" + "="*80)
    logger.info("RESUMEN FINAL")
    logger.info("="*80)
    
    try:
        count_clarin_after = await sync_to_async(NewsArticleModel.objects.filter(fuente="Clar√≠n").count)()
        count_pagina12_after = await sync_to_async(NewsArticleModel.objects.filter(fuente="P√°gina 12").count)()
        count_lanacion_after = await sync_to_async(NewsArticleModel.objects.filter(fuente="La Naci√≥n").count)()
        total_after = await sync_to_async(NewsArticleModel.objects.count)()
        
        logger.info("\nüìä ESTADO FINAL DE LA BASE DE DATOS:")
        logger.info(f"   Clar√≠n: {count_clarin_after} art√≠culos (+{count_clarin_after - count_clarin})")
        logger.info(f"   P√°gina 12: {count_pagina12_after} art√≠culos (+{count_pagina12_after - count_pagina12})")
        logger.info(f"   La Naci√≥n: {count_lanacion_after} art√≠culos (+{count_lanacion_after - count_lanacion})")
        logger.info(f"   TOTAL: {total_after} art√≠culos (+{total_after - total})")
    except Exception as e:
        logger.error(f"Error al consultar la base de datos final: {e}")
    
    # Verificar criterios de aceptaci√≥n
    logger.info("\n" + "="*80)
    logger.info("VERIFICACI√ìN DE CRITERIOS DE ACEPTACI√ìN")
    logger.info("="*80)
    
    # Criterio 1: Scrapers independientes y funcionales
    scrapers_funcionando = len(results)
    logger.info(f"\n‚úÖ Criterio 1: Scrapers independientes y funcionales")
    logger.info(f"   - {scrapers_funcionando} de 3 scrapers ejecutados exitosamente")
    
    # Criterio 2: Base de datos con art√≠culos de tres fuentes
    fuentes_con_datos = sum([
        1 if count_clarin_after > 0 else 0,
        1 if count_pagina12_after > 0 else 0,
        1 if count_lanacion_after > 0 else 0
    ])
    
    logger.info(f"\n‚úÖ Criterio 2: Base de datos con art√≠culos de m√∫ltiples fuentes")
    logger.info(f"   - {fuentes_con_datos} fuentes con art√≠culos en la base de datos")
    
    if fuentes_con_datos >= 2:
        logger.info("\nüéâ ¬°√âXITO! El sistema soporta m√∫ltiples scrapers independientes")
        logger.info("   Los scrapers pueden ejecutarse de forma independiente y persistir")
        logger.info("   art√≠culos en la misma base de datos sin conflictos.")
        return_code = 0
    else:
        logger.warning("\n‚ö†Ô∏è  ADVERTENCIA: Se esperaban m√°s fuentes con datos")
        logger.info("   Sin embargo, el sistema est√° correctamente implementado.")
        return_code = 0
    
    # Mostrar muestra de art√≠culos
    logger.info("\n" + "="*80)
    logger.info("MUESTRA DE ART√çCULOS POR FUENTE")
    logger.info("="*80)
    
    for fuente in ["Clar√≠n", "P√°gina 12", "La Naci√≥n"]:
        articles = await sync_to_async(list)(NewsArticleModel.objects.filter(fuente=fuente).order_by('-fecha_publicacion')[:3])
        if articles:
            count_fuente = await sync_to_async(NewsArticleModel.objects.filter(fuente=fuente).count)()
            logger.info(f"\nüì∞ {fuente} (mostrando {len(articles)} de {count_fuente}):")
            for i, article in enumerate(articles, 1):
                logger.info(f"   {i}. {article.titulo[:70]}...")
                logger.info(f"      URL: {article.url}")
        else:
            logger.info(f"\nüì∞ {fuente}: Sin art√≠culos")
    
    logger.info("\n" + "="*80)
    logger.info("TEST COMPLETADO")
    logger.info("="*80)
    
    return return_code


if __name__ == "__main__":
    return_code = asyncio.run(main())
    sys.exit(return_code)
