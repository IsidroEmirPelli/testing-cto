# ğŸ•·ï¸ Scraper de La NaciÃ³n - DocumentaciÃ³n

## ğŸ“‹ Resumen

ImplementaciÃ³n completa de un scraper especÃ­fico para el sitio web de La NaciÃ³n que extrae artÃ­culos recientes y los persiste en la base de datos, evitando duplicados.

## âœ… CaracterÃ­sticas Implementadas

### 1. Scraper (LaNacionScraper)
- âœ… Implementa la interfaz `ScraperPort` mediante Protocol typing
- âœ… Extrae artÃ­culos de mÃºltiples secciones:
  - PolÃ­tica
  - EconomÃ­a
  - Sociedad
- âœ… Retorna objetos `ArticleDTO` estandarizados
- âœ… Manejo robusto de errores y logging
- âœ… ConfiguraciÃ³n flexible (max_articles, timeout)

### 2. ExtracciÃ³n de Datos
- âœ… **TÃ­tulo**: MÃºltiples selectores de fallback
- âœ… **Contenido**: ExtracciÃ³n de pÃ¡rrafos con normalizaciÃ³n de texto
- âœ… **URL**: URLs absolutas y completas
- âœ… **Fecha de PublicaciÃ³n**: ExtracciÃ³n de metadatos o fecha actual
- âœ… **Fuente**: "La NaciÃ³n" hardcodeado
- âœ… **NormalizaciÃ³n**: Limpieza de espacios, saltos de lÃ­nea y caracteres especiales

### 3. Caso de Uso (ScrapeAndPersistArticlesUseCase)
- âœ… Orquesta el flujo completo de scraping y persistencia
- âœ… Verifica duplicados por URL antes de insertar
- âœ… Utiliza el repositorio de NewsArticle para persistencia
- âœ… Retorna estadÃ­sticas detalladas del proceso

### 4. PrevenciÃ³n de Duplicados
- âœ… Consulta la base de datos por URL antes de insertar
- âœ… Logging de artÃ­culos duplicados omitidos
- âœ… Contador de duplicados en las estadÃ­sticas

### 5. Testing
- âœ… Tests unitarios completos implementados
- âœ… Tests de inicializaciÃ³n
- âœ… Tests de extracciÃ³n de tÃ­tulo, contenido y fecha
- âœ… Tests de normalizaciÃ³n de texto
- âœ… Tests de manejo de errores
- âœ… Tests de filtrado de URLs
- âœ… Tests de integraciÃ³n con base de datos
- âœ… Script de prueba funcional (`test_lanacion_scraper.py`)

## ğŸ“ Estructura de Archivos

```
src/infrastructure/adapters/scrapers/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ clarin_scraper.py
â”œâ”€â”€ pagina12_scraper.py
â””â”€â”€ lanacion_scraper.py              # Scraper de La NaciÃ³n

src/application/use_cases/
â””â”€â”€ scrape_and_persist_articles.py # Caso de uso de scraping + persistencia

tests/unit/
â””â”€â”€ test_lanacion_scraper.py         # Tests unitarios

tests/integration/
â””â”€â”€ test_lanacion_scraper_integration.py  # Tests de integraciÃ³n

test_lanacion_scraper.py             # Script de prueba funcional
```

## ğŸš€ Uso

### Uso BÃ¡sico

```python
from src.infrastructure.adapters.scrapers.lanacion_scraper import LaNacionScraper

# Crear instancia del scraper
scraper = LaNacionScraper(max_articles=15)

# Ejecutar scraping
articles = scraper.scrape()

# Procesar artÃ­culos
for article in articles:
    print(f"{article.titulo} - {article.url}")
```

### Uso con Persistencia

```python
import asyncio
from src.infrastructure.adapters.scrapers.lanacion_scraper import LaNacionScraper
from src.infrastructure.persistence.django_repositories import DjangoNewsArticleRepository
from src.application.use_cases.scrape_and_persist_articles import ScrapeAndPersistArticlesUseCase

async def main():
    # Inicializar componentes
    scraper = LaNacionScraper(max_articles=15)
    repository = DjangoNewsArticleRepository()
    use_case = ScrapeAndPersistArticlesUseCase(scraper, repository)
    
    # Ejecutar
    result = await use_case.execute()
    
    print(f"Total scrapeados: {result['total_scraped']}")
    print(f"Nuevos insertados: {result['total_new']}")
    print(f"Duplicados omitidos: {result['total_duplicates']}")

asyncio.run(main())
```

### Script de Prueba

```bash
# Ejecutar el script de prueba funcional
python test_lanacion_scraper.py

# El script realiza:
# 1. Scrapea artÃ­culos de La NaciÃ³n
# 2. Los persiste en la base de datos
# 3. Verifica duplicados
# 4. Muestra estadÃ­sticas y muestra de artÃ­culos
```

## ğŸ”§ ConfiguraciÃ³n

### ParÃ¡metros del Scraper

```python
scraper = LaNacionScraper(
    max_articles=15,  # NÃºmero mÃ¡ximo de artÃ­culos a extraer
    timeout=30        # Timeout para peticiones HTTP (segundos)
)
```

### Secciones ExtraÃ­das

El scraper extrae artÃ­culos de las siguientes secciones:

1. **PolÃ­tica**: `https://www.lanacion.com.ar/politica/`
2. **EconomÃ­a**: `https://www.lanacion.com.ar/economia/`
3. **Sociedad**: `https://www.lanacion.com.ar/sociedad/`

## ğŸ“Š Flujo de ExtracciÃ³n

### Fase 1: RecolecciÃ³n de URLs

1. Accede a cada secciÃ³n configurada
2. Extrae URLs de artÃ­culos de:
   - Tags `<article>` con enlaces
   - TÃ­tulos `<h2>` y `<h3>` con enlaces
3. Filtra URLs no deseadas (temas, autores, secciones)
4. Elimina duplicados
5. Limita a `max_articles`

### Fase 2: ExtracciÃ³n de Contenido

Para cada URL:

1. **TÃ­tulo**: Busca en selectores especÃ­ficos de La NaciÃ³n
   - `h1.com-title`
   - `h1.title`
   - `h1.nota-title`
   - `h1` genÃ©rico
   - Meta tag `og:title`

2. **Contenido**: Extrae pÃ¡rrafos de:
   - `div.nota`
   - `div.contenido`
   - `div.article-body`
   - Tags `<article>`

3. **Fecha de PublicaciÃ³n**: Extrae de:
   - Meta tag `article:published_time`
   - Meta tag `publishdate`
   - Tag `<time>` con atributo `datetime`
   - Spans con clase `fecha` o `date`
   - Default: fecha actual si no se encuentra

4. **NormalizaciÃ³n**: Limpia el texto extraÃ­do
   - Elimina espacios mÃºltiples
   - Elimina saltos de lÃ­nea
   - Elimina caracteres especiales

## ğŸ§ª Testing

### Ejecutar Tests Unitarios

```bash
pytest tests/unit/test_lanacion_scraper.py -v
```

### Ejecutar Tests de IntegraciÃ³n

```bash
pytest tests/integration/test_lanacion_scraper_integration.py -v
```

### Tests Implementados

**Tests Unitarios:**
- `test_scraper_initialization`: InicializaciÃ³n correcta
- `test_scraper_default_initialization`: Valores por defecto
- `test_scrape_returns_list`: Tipo de retorno
- `test_extract_title_from_different_selectors`: ExtracciÃ³n de tÃ­tulos
- `test_extract_content_from_paragraphs`: ExtracciÃ³n de contenido
- `test_extract_publication_date_from_meta`: ExtracciÃ³n de fechas
- `test_extract_publication_date_defaults_to_now`: Fecha por defecto
- `test_article_dto_has_required_fields`: ValidaciÃ³n de DTO
- `test_scraper_handles_network_errors_gracefully`: Manejo de errores
- `test_scraper_filters_unwanted_urls`: Filtrado de URLs
- `test_clean_text_removes_extra_whitespace`: NormalizaciÃ³n de espacios
- `test_clean_text_removes_newlines`: NormalizaciÃ³n de saltos de lÃ­nea

**Tests de IntegraciÃ³n:**
- `test_complete_scraping_flow`: Flujo completo de scraping y persistencia
- `test_duplicate_detection`: DetecciÃ³n de duplicados
- `test_scraper_handles_real_website`: Prueba con sitio real

## ğŸ” Manejo de Errores

El scraper implementa manejo robusto de errores:

### Errores de Red
- Timeout en peticiones HTTP
- Errores de conexiÃ³n
- Respuestas HTTP no exitosas (4xx, 5xx)

### Errores de Parsing
- HTML malformado
- Selectores no encontrados
- Encoding incorrecto

### Logging
- Nivel INFO para operaciones normales
- Nivel WARNING para artÃ­culos sin tÃ­tulo/contenido
- Nivel ERROR para errores de red o parsing

## ğŸ“ˆ EstadÃ­sticas y Resultados

El caso de uso retorna un diccionario con:

```python
{
    'total_scraped': int,      # Total de artÃ­culos scrapeados
    'total_new': int,          # ArtÃ­culos nuevos insertados
    'total_duplicates': int,   # ArtÃ­culos duplicados omitidos
    'articles': list[Entity]   # Lista de artÃ­culos (entidades)
}
```

## ğŸ¯ Criterios de AceptaciÃ³n

âœ… **Scraper independiente y funcional**
- El scraper funciona de manera autÃ³noma
- No depende de implementaciones especÃ­ficas de otros scrapers
- Sigue el mismo patrÃ³n que ClarÃ­n y PÃ¡gina 12

âœ… **MÃºltiples secciones**
- Extrae de PolÃ­tica, EconomÃ­a y Sociedad
- Hasta 15 artÃ­culos por defecto

âœ… **Manejo de encoding**
- Utiliza BeautifulSoup con parser 'lxml'
- Maneja correctamente UTF-8 y otros encodings

âœ… **NormalizaciÃ³n de contenido**
- Limpia espacios mÃºltiples
- Elimina saltos de lÃ­nea
- Normaliza texto extraÃ­do

âœ… **PrevenciÃ³n de duplicados**
- Verifica URLs antes de insertar
- No inserta artÃ­culos duplicados

âœ… **Base de datos con tres fuentes**
- La base de datos ahora contiene artÃ­culos de:
  - ClarÃ­n
  - PÃ¡gina 12
  - La NaciÃ³n

## ğŸ”— URLs de Ejemplo

```
https://www.lanacion.com.ar/politica/
https://www.lanacion.com.ar/economia/
https://www.lanacion.com.ar/sociedad/
```

## ğŸš€ PrÃ³ximos Pasos

1. **Configurar cron jobs**: Ejecutar el scraper periÃ³dicamente
2. **Monitoreo**: Implementar alertas para errores de scraping
3. **Rate limiting**: Agregar delays entre peticiones si es necesario
4. **CachÃ©**: Implementar cachÃ© de URLs ya procesadas
5. **Scrapers adicionales**: Agregar mÃ¡s fuentes de noticias

## ğŸ“ Notas TÃ©cnicas

### Diferencias con Scrapers Anteriores

- Similar a ClarÃ­n y PÃ¡gina 12 en estructura
- Incluye mÃ©todo `_clean_text()` para normalizaciÃ³n
- Busca tambiÃ©n en tags `<h2>` y `<h3>` para URLs
- Filtros especÃ­ficos para URLs de La NaciÃ³n

### Consideraciones de Performance

- Sesiones HTTP reutilizadas para eficiencia
- Timeout configurable para evitar bloqueos
- Logging detallado para debugging
- Manejo graceful de errores sin interrumpir el proceso

### Compatibilidad

- Python 3.12+
- Django ORM para persistencia
- BeautifulSoup4 con lxml parser
- Requests para peticiones HTTP
