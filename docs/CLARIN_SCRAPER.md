# ğŸ•·ï¸ Scraper de ClarÃ­n - DocumentaciÃ³n

## ğŸ“‹ Resumen

ImplementaciÃ³n completa de un scraper especÃ­fico para el sitio web de ClarÃ­n que extrae artÃ­culos recientes y los persiste en la base de datos, evitando duplicados.

## âœ… CaracterÃ­sticas Implementadas

### 1. Scraper (ClarinScraper)
- âœ… Implementa la interfaz `ScraperPort` mediante Protocol typing
- âœ… Extrae artÃ­culos de mÃºltiples secciones:
  - Ãšltimas Noticias
  - PolÃ­tica
  - EconomÃ­a
- âœ… Retorna objetos `ArticleDTO` estandarizados
- âœ… Manejo robusto de errores y logging
- âœ… ConfiguraciÃ³n flexible (max_articles, timeout)

### 2. ExtracciÃ³n de Datos
- âœ… **TÃ­tulo**: MÃºltiples selectores de fallback
- âœ… **Contenido**: ExtracciÃ³n de pÃ¡rrafos y texto completo
- âœ… **URL**: URLs absolutas y completas
- âœ… **Fecha de PublicaciÃ³n**: ExtracciÃ³n de metadatos o fecha actual
- âœ… **Fuente**: "ClarÃ­n" hardcodeado

### 3. Caso de Uso (ScrapeAndPersistArticlesUseCase)
- âœ… Orquesta el flujo completo de scraping y persistencia
- âœ… Verifica duplicados por URL antes de insertar
- âœ… Utiliza el repositorio de NewsArticle para persistencia
- âœ… Retorna estadÃ­sticas detalladas del proceso

### 4. PrevenciÃ³n de Duplicados
- âœ… Consulta la base de datos por URL antes de insertar
- âœ… Logging de artÃ­culos duplicados omitidos
- âœ… Contador de duplicados en las estadÃ­sticas

### 5. Testing
- âœ… 10 tests unitarios implementados
- âœ… Tests de inicializaciÃ³n
- âœ… Tests de extracciÃ³n de tÃ­tulo, contenido y fecha
- âœ… Tests de manejo de errores
- âœ… Tests de filtrado de URLs
- âœ… Script de prueba funcional (`test_clarin_scraper.py`)

## ğŸ“ Estructura de Archivos

```
src/infrastructure/adapters/scrapers/
â”œâ”€â”€ __init__.py
â””â”€â”€ clarin_scraper.py              # Scraper de ClarÃ­n

src/application/use_cases/
â””â”€â”€ scrape_and_persist_articles.py # Caso de uso de scraping + persistencia

tests/unit/
â””â”€â”€ test_clarin_scraper.py         # Tests unitarios

test_clarin_scraper.py             # Script de prueba funcional
```

## ğŸš€ Uso

### Uso BÃ¡sico

```python
from src.infrastructure.adapters.scrapers.clarin_scraper import ClarinScraper

# Crear instancia del scraper
scraper = ClarinScraper(max_articles=15)

# Ejecutar scraping
articles = scraper.scrape()

# Procesar artÃ­culos
for article in articles:
    print(f"{article.titulo} - {article.url}")
```

### Uso con Persistencia

```python
import asyncio
from src.infrastructure.adapters.scrapers.clarin_scraper import ClarinScraper
from src.infrastructure.persistence.django_repositories import DjangoNewsArticleRepository
from src.application.use_cases.scrape_and_persist_articles import ScrapeAndPersistArticlesUseCase

async def main():
    # Inicializar componentes
    scraper = ClarinScraper(max_articles=15)
    repository = DjangoNewsArticleRepository()
    use_case = ScrapeAndPersistArticlesUseCase(scraper, repository)
    
    # Ejecutar
    result = await use_case.execute()
    
    print(f"Total scrapeados: {result['total_scraped']}")
    print(f"Nuevos insertados: {result['total_new']}")
    print(f"Duplicados omitidos: {result['total_duplicates']}")

asyncio.run(main())
```

### Uso del Script de Prueba

```bash
# Activar entorno virtual
source venv/bin/activate

# Ejecutar test del scraper
python test_clarin_scraper.py
```

## ğŸ”§ ConfiguraciÃ³n

### ParÃ¡metros del Scraper

```python
ClarinScraper(
    max_articles=15,  # NÃºmero mÃ¡ximo de artÃ­culos a extraer
    timeout=30        # Timeout para peticiones HTTP (segundos)
)
```

### Secciones Scrapeadas

El scraper extrae artÃ­culos de las siguientes secciones:
- `/ultimas-noticias/`
- `/politica/`
- `/economia/`

## ğŸ“Š Resultados de Pruebas

### Test Funcional
```
âœ… Total de artÃ­culos scrapeados: 15
âœ… ArtÃ­culos nuevos insertados: 15 (primera ejecuciÃ³n)
âœ… ArtÃ­culos duplicados omitidos: 0 (primera ejecuciÃ³n)

Segunda ejecuciÃ³n:
âœ… Total de artÃ­culos scrapeados: 15
âœ… ArtÃ­culos nuevos insertados: 6
âœ… ArtÃ­culos duplicados omitidos: 9
```

### Tests Unitarios
```
âœ… 10/10 tests pasando
- test_scraper_initialization
- test_scraper_default_initialization
- test_scrape_returns_list
- test_extract_title_from_different_selectors
- test_extract_content_from_paragraphs
- test_extract_publication_date_from_meta
- test_extract_publication_date_defaults_to_now
- test_article_dto_has_required_fields
- test_scraper_handles_network_errors_gracefully
- test_scraper_filters_unwanted_urls
```

## ğŸ¯ Criterios de AceptaciÃ³n

- [x] Scraper funcional que extrae artÃ­culos reales de ClarÃ­n
- [x] Implementa la interfaz ScraperPort
- [x] Extrae tÃ­tulo, contenido, URL y fecha
- [x] Evita duplicados al insertar en la base de datos
- [x] Manejo apropiado de logs y errores
- [x] Al menos 10 artÃ­culos reales en la base de datos

## ğŸ—ï¸ Arquitectura

### Principios Seguidos

1. **Arquitectura Hexagonal**: El scraper es un adaptador en la capa de infraestructura
2. **Protocol Typing**: Uso de `ScraperPort` Protocol para tipado estructural
3. **SeparaciÃ³n de Responsabilidades**: 
   - `ClarinScraper`: ExtracciÃ³n de datos
   - `ScrapeAndPersistArticlesUseCase`: LÃ³gica de negocio y persistencia
4. **Dependency Inversion**: El caso de uso depende de interfaces, no de implementaciones concretas

### Flujo de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ClarinScraper  â”‚ (Adaptador)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ scrape()
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ArticleDTO    â”‚ (DTO)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ScrapeAndPersistArticles    â”‚ (Use Case)
â”‚   UseCase                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ check duplicates
         â”‚ create entities
         â”‚ persist
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DjangoNewsArticleRepository â”‚ (Repository)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Database     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› Manejo de Errores

El scraper implementa mÃºltiples niveles de manejo de errores:

1. **Errores de Red**: Capturados y loggeados, continÃºa con otros artÃ­culos
2. **Errores de Parsing**: Intentos con mÃºltiples selectores de fallback
3. **ArtÃ­culos Sin TÃ­tulo**: Omitidos con warning en el log
4. **Duplicados en DB**: Detectados y omitidos sin error

## ğŸ“ Logging

El scraper proporciona logging detallado:

```
INFO - ClarinScraper inicializado - max_articles: 15
INFO - Iniciando scraping de ClarÃ­n
INFO - Extrayendo URLs de secciÃ³n: https://www.clarin.com/ultimas-noticias/
INFO - URLs extraÃ­das de /ultimas-noticias/: 8
INFO - Extrayendo contenido de 15 artÃ­culos
INFO - ArtÃ­culo extraÃ­do exitosamente: TÃ­tulo del artÃ­culo...
INFO - Scraping completado. Total de artÃ­culos extraÃ­dos: 15
INFO - ArtÃ­culos nuevos insertados: 15
INFO - ArtÃ­culos duplicados (omitidos): 0
```

## ğŸ”„ Mejoras Futuras

Posibles mejoras para el scraper:

1. **ExtracciÃ³n de CategorÃ­as**: Parsear la categorÃ­a desde el breadcrumb
2. **ExtracciÃ³n de ImÃ¡genes**: Agregar URLs de imÃ¡genes principales
3. **ExtracciÃ³n de Autores**: Identificar autores de los artÃ­culos
4. **Rate Limiting**: Implementar delays entre peticiones
5. **Cache de SesiÃ³n**: Mantener cookies entre ejecuciones
6. **Scraping Incremental**: Solo scrapear artÃ­culos nuevos desde Ãºltima ejecuciÃ³n
7. **ValidaciÃ³n de Contenido**: Verificar que el contenido tenga longitud mÃ­nima

## ğŸ“š Referencias

- **DocumentaciÃ³n de BeautifulSoup**: https://www.crummy.com/software/BeautifulSoup/
- **Requests Library**: https://docs.python-requests.org/
- **Arquitectura Hexagonal**: Documentada en `docs/ARCHITECTURE.md`
- **ScraperPort Interface**: `src/domain/ports/scraper_port.py`

## ğŸ‘¥ Desarrollo

Implementado siguiendo los principios de:
- Clean Code
- SOLID
- Arquitectura Hexagonal
- Test-Driven Development (TDD)

---

**Estado**: âœ… Completado
**Fecha**: Octubre 2025
**Ticket**: #5 - Scraper ClarÃ­n
