# ğŸ“° Scraper de PÃ¡gina 12 - DocumentaciÃ³n TÃ©cnica

## ğŸ¯ PropÃ³sito

ImplementaciÃ³n de un scraper para extraer artÃ­culos del sitio web de PÃ¡gina 12 (https://www.pagina12.com.ar), siguiendo el mismo patrÃ³n arquitectÃ³nico que el scraper de ClarÃ­n.

## ğŸ—ï¸ Arquitectura

### ImplementaciÃ³n del ScraperPort

El `Pagina12Scraper` implementa la interfaz `ScraperPort` mediante Protocol typing estructural:

```python
class Pagina12Scraper:
    def scrape(self) -> list[ArticleDTO]:
        """Extrae artÃ­culos de PÃ¡gina 12"""
```

### Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Pagina12Scraper                     â”‚
â”‚  (Infrastructure/Adapters/Scrapers)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”œâ”€â–º _extract_article_urls_from_section()
                    â”œâ”€â–º _extract_article_content()
                    â”œâ”€â–º _extract_title()
                    â”œâ”€â–º _extract_content()
                    â”œâ”€â–º _extract_publication_date()
                    â””â”€â–º _clean_text()
```

## ğŸ” Funcionalidad

### Secciones Scrapeadas

1. **El PaÃ­s** (`/secciones/el-pais`) - Noticias nacionales
2. **EconomÃ­a** (`/secciones/economia`) - Noticias econÃ³micas
3. **Sociedad** (`/secciones/sociedad`) - Noticias de sociedad

### Campos ExtraÃ­dos

| Campo | DescripciÃ³n | Fuente HTML |
|-------|-------------|-------------|
| `titulo` | TÃ­tulo del artÃ­culo | `<h1>`, `<meta property="og:title">` |
| `url` | URL completa del artÃ­culo | URLs con `/notas/` o `/articulos/` |
| `contenido` | Texto completo del artÃ­culo | `<div class="article-text">`, `<p>` |
| `fecha_publicacion` | Fecha y hora de publicaciÃ³n | `<meta property="article:published_time">` |
| `fuente` | Nombre de la fuente | "PÃ¡gina 12" (hardcoded) |

## ğŸ› ï¸ ImplementaciÃ³n

### InicializaciÃ³n

```python
from src.infrastructure.adapters.scrapers import Pagina12Scraper

# Con valores por defecto
scraper = Pagina12Scraper()

# Con configuraciÃ³n personalizada
scraper = Pagina12Scraper(max_articles=20, timeout=30)
```

**ParÃ¡metros:**
- `max_articles` (int): NÃºmero mÃ¡ximo de artÃ­culos a extraer (default: 15)
- `timeout` (int): Timeout para peticiones HTTP en segundos (default: 30)

### Uso BÃ¡sico

```python
# Extraer artÃ­culos
articles = scraper.scrape()

# Procesar resultados
for article in articles:
    print(f"TÃ­tulo: {article.titulo}")
    print(f"URL: {article.url}")
    print(f"Fuente: {article.fuente}")
```

### Uso con Persistencia

```python
from src.application.use_cases import ScrapeAndPersistArticlesUseCase
from src.infrastructure.persistence.django_app.repositories import DjangoNewsArticleRepository

scraper = Pagina12Scraper()
repository = DjangoNewsArticleRepository()
use_case = ScrapeAndPersistArticlesUseCase(repository)

# Ejecutar scraping y persistencia
result = await use_case.execute(scraper)
print(f"Nuevos: {result['nuevos_insertados']}")
print(f"Duplicados: {result['duplicados']}")
```

## ğŸ”§ Detalles TÃ©cnicos

### Proceso de ExtracciÃ³n

1. **Fase 1: RecolecciÃ³n de URLs**
   - Accede a cada secciÃ³n configurada
   - Busca elementos `<article>` y `<div>` con clases de noticias
   - Filtra URLs no deseadas (autores, tags, suplementos)
   - Valida que las URLs contengan `/notas/` o `/articulos/`

2. **Fase 2: ExtracciÃ³n de Contenido**
   - Para cada URL recolectada:
     - Extrae tÃ­tulo usando mÃºltiples selectores
     - Extrae contenido de pÃ¡rrafos (filtra pÃ¡rrafos < 20 caracteres)
     - Extrae fecha de publicaciÃ³n
     - Limpia y normaliza el texto
     - Crea ArticleDTO

### Selectores HTML

#### TÃ­tulos
```python
title_selectors = [
    ('h1', {'class': 'article-title'}),
    ('h1', {'class': 'titulo-nota'}),
    ('h1', {'class': 'title'}),
    ('h1', {}),
    ('meta', {'property': 'og:title'}),
]
```

#### Contenido
```python
content_selectors = [
    ('div', {'class': 'article-text'}),
    ('div', {'class': 'article-body'}),
    ('div', {'class': 'texto-nota'}),
    ('div', {'class': 'content-nota'}),
    ('article', {'class': 'article'}),
]
```

#### Fechas
```python
date_selectors = [
    ('meta', {'property': 'article:published_time'}),
    ('meta', {'name': 'publishdate'}),
    ('meta', {'property': 'og:published_time'}),
    ('time', {'datetime': True}),
    ('span', {'class': 'date'}),
]
```

### Limpieza de Texto

El mÃ©todo `_clean_text()` reutiliza la lÃ³gica del scraper de ClarÃ­n:

```python
def _clean_text(self, text: str) -> str:
    """Limpia y normaliza el texto extraÃ­do"""
    if not text:
        return ""
    
    # Eliminar espacios mÃºltiples
    text = ' '.join(text.split())
    
    # Eliminar saltos de lÃ­nea y caracteres especiales
    text = text.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
    
    # Eliminar espacios mÃºltiples resultantes
    text = ' '.join(text.split())
    
    return text.strip()
```

## ğŸ“Š Manejo de Errores

### Errores de Red

```python
try:
    response = self.session.get(url, timeout=self.timeout)
    response.raise_for_status()
except requests.RequestException as e:
    logger.error(f"Error de red: {e}")
    return None
```

### Errores de Parsing

```python
try:
    soup = BeautifulSoup(response.content, 'lxml')
    titulo = self._extract_title(soup)
    if not titulo:
        logger.warning(f"No se pudo extraer tÃ­tulo")
        return None
except Exception as e:
    logger.error(f"Error inesperado: {e}", exc_info=True)
    return None
```

### Logging

El scraper utiliza logging estructurado:

```python
logger.info("Iniciando scraping de PÃ¡gina 12")
logger.info(f"Extrayendo URLs de secciÃ³n: {section_url}")
logger.info(f"URLs extraÃ­das: {len(urls)}")
logger.info(f"ArtÃ­culo extraÃ­do: {titulo[:60]}...")
logger.error(f"Error extrayendo artÃ­culo {url}: {e}", exc_info=True)
logger.info(f"Scraping completado. Total: {len(articles)}")
```

## ğŸ§ª Testing

### Tests Unitarios

UbicaciÃ³n: `tests/unit/test_pagina12_scraper.py`

Tests implementados:
- âœ… InicializaciÃ³n del scraper
- âœ… ExtracciÃ³n de tÃ­tulos con diferentes selectores
- âœ… ExtracciÃ³n de contenido de pÃ¡rrafos
- âœ… Filtrado de pÃ¡rrafos cortos
- âœ… ExtracciÃ³n de fechas
- âœ… Manejo de errores de red
- âœ… Filtrado de URLs no deseadas
- âœ… Limpieza de texto
- âœ… Conformidad con ScraperPort

```bash
# Ejecutar tests unitarios
python -m pytest tests/unit/test_pagina12_scraper.py -v
```

### Tests de IntegraciÃ³n

UbicaciÃ³n: `tests/integration/test_pagina12_scraper_integration.py`

Tests implementados:
- âœ… ExtracciÃ³n de artÃ­culos del sitio real
- âœ… Flujo completo de scraping y persistencia
- âœ… DetecciÃ³n de duplicados

```bash
# Ejecutar tests de integraciÃ³n
python -m pytest tests/integration/test_pagina12_scraper_integration.py -v
```

### Script de Prueba Funcional

UbicaciÃ³n: `test_pagina12_scraper.py`

```bash
# Ejecutar script de prueba
python test_pagina12_scraper.py
```

Salida esperada:
```
====================================================================
TEST FUNCIONAL: Scraper de PÃ¡gina 12
====================================================================

ğŸ“Š ArtÃ­culos de PÃ¡gina 12 en BD antes: 0

ğŸ”§ Inicializando componentes...
âœ… Componentes inicializados

ğŸ•·ï¸  Ejecutando scraping de PÃ¡gina 12...
--------------------------------------------------------------------

====================================================================
RESULTADOS DEL SCRAPING
====================================================================
âœ… Total scrapeado:     15 artÃ­culos
âœ… Nuevos insertados:   15 artÃ­culos
âš ï¸  Duplicados:         0 artÃ­culos
====================================================================

ğŸ“Š ArtÃ­culos de PÃ¡gina 12 en BD despuÃ©s: 15
ğŸ“ˆ Incremento: +15 artÃ­culos

âœ… CRITERIO CUMPLIDO: Al menos 10 artÃ­culos de PÃ¡gina 12 en la base de datos
```

## ğŸ”„ ComparaciÃ³n con ClarinScraper

### Similitudes

| Aspecto | Ambos Scrapers |
|---------|----------------|
| Arquitectura | Implementan ScraperPort |
| Estructura | Dos fases: URLs â†’ Contenido |
| Limpieza | Mismo mÃ©todo `_clean_text()` |
| Errores | Manejo gracioso, logging comprehensivo |
| Testing | Cobertura completa (unit + integration) |

### Diferencias

| Aspecto | ClarinScraper | Pagina12Scraper |
|---------|---------------|-----------------|
| Base URL | `www.clarin.com` | `www.pagina12.com.ar` |
| Secciones | `/ultimas-noticias/`, `/politica/`, `/economia/` | `/secciones/el-pais`, `/secciones/economia`, `/secciones/sociedad` |
| URLs vÃ¡lidas | Cualquier URL de ClarÃ­n | Solo con `/notas/` o `/articulos/` |
| Filtros | `/tema/`, `/tags/`, `/autor/` | `/autores/`, `/tags/`, `/suplementos/` |
| Selectores | EspecÃ­ficos de ClarÃ­n | EspecÃ­ficos de PÃ¡gina 12 |
| Filtro contenido | Sin filtro por longitud | Filtra pÃ¡rrafos < 20 caracteres |

## ğŸ“ CaracterÃ­sticas Destacadas

### 1. ReutilizaciÃ³n de CÃ³digo
- âœ… Mismo patrÃ³n arquitectÃ³nico que ClarinScraper
- âœ… ReutilizaciÃ³n de lÃ³gica de limpieza de texto
- âœ… Mismo flujo de persistencia

### 2. ValidaciÃ³n de Arquitectura
- âœ… Demuestra extensibilidad del sistema
- âœ… Nuevo scraper sin modificar dominio
- âœ… Conformidad con ScraperPort

### 3. Filtrado Inteligente
- âœ… Filtra URLs no deseadas (autores, tags, suplementos)
- âœ… Solo extrae URLs de notas/artÃ­culos
- âœ… Filtra pÃ¡rrafos muy cortos (< 20 caracteres)

### 4. Robustez
- âœ… MÃºltiples selectores de fallback
- âœ… Manejo gracioso de errores
- âœ… Logging comprehensivo
- âœ… Cierre automÃ¡tico de sesiÃ³n

## ğŸš€ Ejemplo de Uso Completo

```python
#!/usr/bin/env python3
import asyncio
import django
import os

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'src.infrastructure.config.django_settings')
django.setup()

from src.infrastructure.adapters.scrapers import Pagina12Scraper
from src.infrastructure.persistence.django_app.repositories import DjangoNewsArticleRepository
from src.application.use_cases import ScrapeAndPersistArticlesUseCase

async def main():
    # Inicializar componentes
    scraper = Pagina12Scraper(max_articles=15)
    repository = DjangoNewsArticleRepository()
    use_case = ScrapeAndPersistArticlesUseCase(repository)
    
    # Ejecutar scraping y persistencia
    result = await use_case.execute(scraper)
    
    # Mostrar resultados
    print(f"Total scrapeado: {result['total_scrapeado']}")
    print(f"Nuevos insertados: {result['nuevos_insertados']}")
    print(f"Duplicados: {result['duplicados']}")

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ“ˆ Mejoras Futuras

### Funcionalidad
- [ ] ExtracciÃ³n de autores
- [ ] ExtracciÃ³n de categorÃ­as/tags
- [ ] ExtracciÃ³n de imÃ¡genes
- [ ] Soporte para mÃ¡s secciones

### Performance
- [ ] Scraping asÃ­ncrono con aiohttp
- [ ] Rate limiting configurable
- [ ] CachÃ© de artÃ­culos
- [ ] Pool de conexiones

### Calidad
- [ ] Fixtures HTML para tests
- [ ] Tests de performance
- [ ] ValidaciÃ³n de contenido
- [ ] EstadÃ­sticas detalladas

## ğŸ“ ConclusiÃ³n

El `Pagina12Scraper` demuestra exitosamente:

1. âœ… **Extensibilidad**: Nuevo scraper sin modificar el dominio
2. âœ… **ReutilizaciÃ³n**: Mismo patrÃ³n y lÃ³gica base
3. âœ… **Conformidad**: Implementa ScraperPort correctamente
4. âœ… **Calidad**: Cobertura completa de tests
5. âœ… **DocumentaciÃ³n**: DocumentaciÃ³n tÃ©cnica detallada

El scraper estÃ¡ listo para producciÃ³n y puede servir como plantilla para implementar scrapers de otras fuentes de noticias.

---

**Fecha**: Octubre 2025  
**Status**: âœ… COMPLETADO  
**Ticket**: #6 - Scraper PÃ¡gina 12
