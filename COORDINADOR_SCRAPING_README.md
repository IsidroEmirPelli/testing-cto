# Coordinador de Scraping - Gu√≠a R√°pida

## üéØ Prop√≥sito

El **Coordinador de Scraping** (`ScrapeAllSourcesUseCase`) es un componente que ejecuta autom√°ticamente el scraping de todas las fuentes activas de noticias (Clar√≠n, P√°gina12, La Naci√≥n) y registra los resultados en la base de datos.

## ‚ú® Caracter√≠sticas

- ‚úÖ Ejecuta scraping de m√∫ltiples fuentes en una sola operaci√≥n
- ‚úÖ Registra cada ejecuci√≥n en la tabla `ScrapingJob` con estad√≠sticas detalladas
- ‚úÖ Filtra autom√°ticamente art√≠culos duplicados
- ‚úÖ Manejo robusto de errores (una fuente que falla no detiene las dem√°s)
- ‚úÖ Logging centralizado y detallado
- ‚úÖ Listo para integraci√≥n con sistemas de programaci√≥n (cron, APScheduler, Celery)
- ‚úÖ Estad√≠sticas consolidadas en tiempo real

## üöÄ Inicio R√°pido

### Opci√≥n 1: Script de Demostraci√≥n

La forma m√°s r√°pida de probar el coordinador:

```bash
# Activar el entorno virtual
source venv/bin/activate

# Ejecutar el script de demostraci√≥n
python demo_scrape_all_sources.py
```

El script mostrar√°:
- Fuentes activas encontradas
- Progreso del scraping por fuente
- Estad√≠sticas consolidadas
- Detalles por cada fuente

### Opci√≥n 2: Uso Program√°tico

```python
import asyncio
from src.application.use_cases import ScrapeAllSourcesUseCase
from src.infrastructure.persistence.django_repositories import (
    DjangoSourceRepository,
    DjangoScrapingJobRepository,
    DjangoNewsArticleRepository,
)

async def main():
    # Crear repositorios
    source_repo = DjangoSourceRepository()
    job_repo = DjangoScrapingJobRepository()
    article_repo = DjangoNewsArticleRepository()
    
    # Crear caso de uso
    scrape_all = ScrapeAllSourcesUseCase(
        source_repository=source_repo,
        scraping_job_repository=job_repo,
        article_repository=article_repo,
    )
    
    # Ejecutar
    result = await scrape_all.execute()
    print(f"Art√≠culos scrapeados: {result['total_articles_scraped']}")
    print(f"Art√≠culos nuevos: {result['total_articles_persisted']}")

asyncio.run(main())
```

## üìä Respuesta del Coordinador

El m√©todo `execute()` retorna un diccionario con estad√≠sticas completas:

```python
{
    'total_sources': 3,                    # Fuentes procesadas
    'total_jobs_completed': 3,             # Jobs exitosos
    'total_jobs_failed': 0,                # Jobs fallidos
    'total_articles_scraped': 45,          # Total art√≠culos encontrados
    'total_articles_persisted': 38,        # Art√≠culos nuevos guardados
    'jobs_details': [                      # Detalle por fuente
        {
            'job_id': 'uuid-here',
            'source': 'Clar√≠n',
            'status': 'completed',
            'fecha_inicio': '2024-01-15T10:00:00Z',
            'fecha_fin': '2024-01-15T10:02:30Z',
            'articles_scraped': 15,
            'articles_persisted': 12,
            'duplicates': 3,
            'error': None
        },
        # ... m√°s fuentes
    ]
}
```

## üìÅ Archivos Importantes

### C√≥digo Principal
- **`src/application/use_cases/scrape_all_sources.py`** - Implementaci√≥n del coordinador
- **`demo_scrape_all_sources.py`** - Script de demostraci√≥n ejecutable
- **`examples_scheduler_integration.py`** - Ejemplos de integraci√≥n con schedulers

### Documentaci√≥n
- **`SCRAPING_COORDINATOR_DOCUMENTATION.md`** - Documentaci√≥n completa y detallada
- **`COORDINADOR_SCRAPING_README.md`** - Esta gu√≠a r√°pida

### Tests
- **`tests/unit/test_scrape_all_sources_use_case.py`** - Suite de tests completa

## üîÑ Integraci√≥n con Schedulers

### Cron (Linux/Unix)

```bash
# Editar crontab
crontab -e

# Ejecutar diariamente a las 6 AM
0 6 * * * cd /path/to/project && source venv/bin/activate && python demo_scrape_all_sources.py >> /var/log/scraping.log 2>&1
```

### APScheduler (Python)

```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler

scheduler = AsyncIOScheduler()
scheduler.add_job(
    execute_scraping,
    'cron',
    hour=6,
    minute=0
)
scheduler.start()
```

### Celery (Distribuido)

```python
from celery import Celery

app = Celery('tasks', broker='redis://localhost')

@app.task
def scrape_task():
    return asyncio.run(execute_scraping())

# Configurar schedule
app.conf.beat_schedule = {
    'daily-scraping': {
        'task': 'tasks.scrape_task',
        'schedule': crontab(hour=6, minute=0),
    },
}
```

Ver `examples_scheduler_integration.py` para m√°s ejemplos detallados.

## üìù Registro de Scraping Jobs

Cada ejecuci√≥n crea registros en la tabla `ScrapingJob`:

| Campo | Descripci√≥n |
|-------|-------------|
| `id` | UUID √∫nico del job |
| `fuente` | Nombre de la fuente (Clar√≠n, P√°gina12, La Naci√≥n) |
| `fecha_inicio` | Timestamp de inicio |
| `fecha_fin` | Timestamp de finalizaci√≥n |
| `status` | Estado: `pending`, `running`, `completed`, `failed` |
| `total_articulos` | Cantidad de art√≠culos scrapeados |
| `created_at` | Fecha de creaci√≥n del registro |
| `updated_at` | Fecha de √∫ltima actualizaci√≥n |

## üêõ Debugging y Logs

### Configurar Logging Detallado

```python
import logging

# Para ver todos los detalles
logging.basicConfig(level=logging.DEBUG)

# Solo para el coordinador
logging.getLogger('src.application.use_cases.scrape_all_sources').setLevel(logging.DEBUG)

# Solo para scrapers
logging.getLogger('src.infrastructure.adapters.scrapers').setLevel(logging.DEBUG)
```

### Ver Logs en Tiempo Real

```bash
# Durante la ejecuci√≥n
python demo_scrape_all_sources.py 2>&1 | tee scraping_$(date +%Y%m%d_%H%M%S).log

# Ver el archivo de log
tail -f scraping_coordinator.log
```

## üß™ Testing

```bash
# Ejecutar tests del coordinador
pytest tests/unit/test_scrape_all_sources_use_case.py -v

# Con cobertura
pytest tests/unit/test_scrape_all_sources_use_case.py --cov=src.application.use_cases.scrape_all_sources --cov-report=html

# Ver reporte de cobertura
open htmlcov/index.html
```

## üîß Configuraci√≥n

### Ajustar Cantidad de Art√≠culos

Por defecto, cada scraper extrae hasta 15 art√≠culos. Para cambiar:

```python
# En scrape_all_sources.py, l√≠nea ~45
self._scraper_factory = {
    "Clar√≠n": lambda: ClarinScraper(max_articles=30),        # Aumentar a 30
    "P√°gina12": lambda: Pagina12Scraper(max_articles=30),
    "La Naci√≥n": lambda: LaNacionScraper(max_articles=30),
}
```

### Agregar Nueva Fuente

1. Crear el scraper (implementar `ScraperPort`)
2. Agregar al factory en `ScrapeAllSourcesUseCase.__init__`
3. Registrar la fuente en la base de datos

Ver documentaci√≥n completa para detalles.

## üìö Documentaci√≥n Adicional

- **Documentaci√≥n Completa**: Ver `SCRAPING_COORDINATOR_DOCUMENTATION.md`
- **Ejemplos de Integraci√≥n**: Ver `examples_scheduler_integration.py`
- **Tests**: Ver `tests/unit/test_scrape_all_sources_use_case.py`

## ‚ö° Tips de Rendimiento

1. **Rate Limiting**: Los scrapers ya incluyen delays razonables
2. **Timeouts**: Configurados a 30 segundos por request
3. **Connection Pooling**: Usa `requests.Session()` para eficiencia
4. **Error Handling**: Un error en una fuente no detiene las dem√°s
5. **Logging**: Use nivel INFO en producci√≥n, DEBUG solo para troubleshooting

## üîí Consideraciones de Seguridad

- Respetar robots.txt de cada sitio
- No ejecutar con demasiada frecuencia (recomendado: m√°ximo cada 4 horas)
- Usar user agents realistas
- Implementar rate limiting si es necesario
- Monitorear logs de errores

## üí° Casos de Uso Comunes

### 1. Scraping Programado Diario
```bash
# Cron para ejecutar todos los d√≠as a las 6 AM
0 6 * * * cd /path/to/project && source venv/bin/activate && python demo_scrape_all_sources.py
```

### 2. Scraping On-Demand via API
```python
@app.post("/api/scraping/execute")
async def execute_scraping_endpoint():
    result = await scrape_all_use_case.execute()
    return result
```

### 3. Monitoreo de Jobs
```python
# Ver √∫ltimos jobs
jobs = await job_repository.get_all(skip=0, limit=10)
for job in jobs:
    print(f"{job.fuente}: {job.status} - {job.total_articulos} art√≠culos")
```

### 4. Estad√≠sticas Hist√≥ricas
```python
# Jobs por fuente
clarin_jobs = await job_repository.get_by_fuente("Clar√≠n")
total_articles = sum(job.total_articulos for job in clarin_jobs)
print(f"Total art√≠culos de Clar√≠n: {total_articles}")
```

## ‚ùì FAQ

**P: ¬øCon qu√© frecuencia debo ejecutar el coordinador?**  
R: Recomendado cada 4-6 horas para evitar sobrecarga de los sitios.

**P: ¬øQu√© pasa si una fuente falla?**  
R: El coordinador registra el error pero contin√∫a con las dem√°s fuentes.

**P: ¬øC√≥mo maneja los duplicados?**  
R: Verifica cada art√≠culo por URL antes de guardar. Los duplicados se omiten autom√°ticamente.

**P: ¬øPuedo ejecutar m√∫ltiples coordinadores en paralelo?**  
R: No recomendado. Usar un solo coordinador con un scheduler apropiado.

**P: ¬øC√≥mo agrego una nueva fuente?**  
R: Ver secci√≥n de extensibilidad en `SCRAPING_COORDINATOR_DOCUMENTATION.md`.

## üÜò Soporte

Para problemas, bugs o preguntas:
- Revisar logs en `scraping_coordinator.log`
- Ejecutar con nivel DEBUG
- Verificar conectividad y acceso a sitios
- Revisar tests para ejemplos de uso

## üìÑ Licencia

Este componente es parte del proyecto de agregador de noticias argentinas.
