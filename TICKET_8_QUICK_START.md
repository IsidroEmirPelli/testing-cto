# üöÄ Ticket 8 - Coordinador de Scraping: Quick Start

## ‚úÖ ¬øQu√© se implement√≥?

Un **coordinador de scraping** que ejecuta autom√°ticamente el scraping de todas las fuentes activas (Clar√≠n, P√°gina12, La Naci√≥n) y registra los resultados en la base de datos.

## üì¶ Archivos Creados

### C√≥digo Principal
- ‚úÖ `src/application/use_cases/scrape_all_sources.py` - Coordinador principal
- ‚úÖ `tests/unit/test_scrape_all_sources_use_case.py` - Tests (10/10 passing)

### Scripts Ejecutables
- ‚úÖ `demo_scrape_all_sources.py` - Script de demostraci√≥n
- ‚úÖ `examples_scheduler_integration.py` - Ejemplos de integraci√≥n
- ‚úÖ `example_api_endpoint.py` - Ejemplo de API REST

### Documentaci√≥n
- ‚úÖ `SCRAPING_COORDINATOR_DOCUMENTATION.md` - Documentaci√≥n completa
- ‚úÖ `COORDINADOR_SCRAPING_README.md` - Gu√≠a r√°pida
- ‚úÖ `TICKET_8_IMPLEMENTATION_SUMMARY.md` - Resumen de implementaci√≥n
- ‚úÖ `TICKET_8_QUICK_START.md` - Este archivo

## üèÉ Probar Ahora (3 pasos)

```bash
# 1. Activar entorno virtual
source venv/bin/activate

# 2. Ejecutar el coordinador
python demo_scrape_all_sources.py

# 3. Ver los resultados
# El script mostrar√° estad√≠sticas completas en consola
```

## üíª Uso Program√°tico

```python
import asyncio
from src.application.use_cases import ScrapeAllSourcesUseCase
from src.infrastructure.persistence.django_repositories import (
    DjangoSourceRepository,
    DjangoScrapingJobRepository,
    DjangoNewsArticleRepository,
)

async def main():
    # Crear caso de uso
    scrape_all = ScrapeAllSourcesUseCase(
        source_repository=DjangoSourceRepository(),
        scraping_job_repository=DjangoScrapingJobRepository(),
        article_repository=DjangoNewsArticleRepository(),
    )
    
    # Ejecutar
    result = await scrape_all.execute()
    
    # Usar resultados
    print(f"Fuentes: {result['total_sources']}")
    print(f"Art√≠culos: {result['total_articles_scraped']}")
    print(f"Nuevos: {result['total_articles_persisted']}")

asyncio.run(main())
```

## üß™ Verificar Tests

```bash
pytest tests/unit/test_scrape_all_sources_use_case.py -v
```

**Resultado esperado**: ‚úÖ 10/10 tests passed

## ‚è∞ Programar Ejecuciones

### Opci√≥n 1: Cron (Simple)
```bash
# Editar crontab
crontab -e

# Agregar: ejecutar diariamente a las 6 AM
0 6 * * * cd /path/to/project && source venv/bin/activate && python demo_scrape_all_sources.py >> /var/log/scraping.log 2>&1
```

### Opci√≥n 2: APScheduler (Avanzado)
Ver `examples_scheduler_integration.py` para c√≥digo completo.

### Opci√≥n 3: Celery (Distribuido)
Ver `examples_scheduler_integration.py` para configuraci√≥n.

## üìä Estructura de Respuesta

```python
{
    'total_sources': 3,                # Fuentes procesadas
    'total_jobs_completed': 3,         # Jobs exitosos
    'total_jobs_failed': 0,            # Jobs fallidos
    'total_articles_scraped': 45,      # Art√≠culos encontrados
    'total_articles_persisted': 38,    # Art√≠culos nuevos guardados
    'jobs_details': [...]              # Detalles por fuente
}
```

## üìù Tabla ScrapingJob

Cada ejecuci√≥n crea registros con:
- `fuente`: Nombre de la fuente
- `status`: pending ‚Üí running ‚Üí completed/failed
- `total_articulos`: Cantidad scrapeada
- `fecha_inicio` / `fecha_fin`: Timestamps
- Y m√°s...

## üîç Consultar Resultados

```python
# Ver √∫ltimos jobs
from src.infrastructure.persistence.django_repositories import DjangoScrapingJobRepository
import asyncio

async def ver_jobs():
    repo = DjangoScrapingJobRepository()
    jobs = await repo.get_all(skip=0, limit=10)
    for job in jobs:
        print(f"{job.fuente}: {job.status} - {job.total_articulos} art√≠culos")

asyncio.run(ver_jobs())
```

## üåê Exponer como API REST

Ver `example_api_endpoint.py` para endpoints completos:
- `POST /api/scraping/execute/` - Ejecutar scraping
- `GET /api/scraping/status/` - Ver estado de jobs
- `GET /api/scraping/statistics/<fuente>/` - Estad√≠sticas por fuente
- `GET /api/scraping/health/` - Health check

## üìö Documentaci√≥n Completa

Para informaci√≥n detallada, ver:

1. **`COORDINADOR_SCRAPING_README.md`**
   - Gu√≠a de usuario
   - Ejemplos b√°sicos
   - FAQ

2. **`SCRAPING_COORDINATOR_DOCUMENTATION.md`**
   - Arquitectura completa
   - Casos de uso avanzados
   - Troubleshooting
   - Mejores pr√°cticas

3. **`TICKET_8_IMPLEMENTATION_SUMMARY.md`**
   - Detalles de implementaci√≥n
   - Decisiones de dise√±o
   - Checklist completo

4. **`examples_scheduler_integration.py`**
   - 7 ejemplos de integraci√≥n
   - C√≥digo copy-paste ready
   - Men√∫ interactivo

5. **`example_api_endpoint.py`**
   - 4 endpoints REST
   - Ejemplos con curl, Python, JavaScript
   - Configuraci√≥n completa

## ‚ú® Caracter√≠sticas Destacadas

- ‚úÖ **Autom√°tico**: Procesa todas las fuentes activas
- ‚úÖ **Resiliente**: Un error no detiene todo
- ‚úÖ **Inteligente**: Filtra duplicados autom√°ticamente
- ‚úÖ **Auditable**: Registra todo en BD
- ‚úÖ **Observable**: Logs detallados
- ‚úÖ **Extensible**: F√°cil agregar fuentes
- ‚úÖ **Testeable**: 100% cubierto por tests

## üéØ Casos de Uso

### Scraping Programado
```bash
# Diariamente a las 6 AM
0 6 * * * python demo_scrape_all_sources.py
```

### Scraping On-Demand
```bash
# Manual cuando sea necesario
python demo_scrape_all_sources.py
```

### Scraping via API
```bash
# Desde cualquier cliente
curl -X POST http://localhost:8000/api/scraping/execute/
```

## üêõ Troubleshooting

### Problema: No se encuentran fuentes
**Soluci√≥n**: Asegurarse que hay fuentes activas en la BD

### Problema: Tests no pasan
**Soluci√≥n**: Activar venv y verificar dependencias
```bash
source venv/bin/activate
pip install -r requirements.txt
```

### Problema: Import errors
**Soluci√≥n**: Asegurarse de estar en el directorio del proyecto
```bash
cd /path/to/project
python -c "from src.application.use_cases import ScrapeAllSourcesUseCase"
```

## üìû Pr√≥ximos Pasos

1. **Probar el coordinador**: `python demo_scrape_all_sources.py`
2. **Ver los tests**: `pytest tests/unit/test_scrape_all_sources_use_case.py -v`
3. **Explorar ejemplos**: `python examples_scheduler_integration.py`
4. **Leer documentaci√≥n**: Ver archivos .md mencionados arriba
5. **Integrar con scheduler**: Elegir cron/APScheduler/Celery seg√∫n necesidad

## ‚úÖ Checklist de Verificaci√≥n

- [x] C√≥digo implementado y testeado
- [x] 10/10 tests passing
- [x] Documentaci√≥n completa
- [x] Scripts de demo funcionales
- [x] Ejemplos de integraci√≥n
- [x] API REST de ejemplo
- [x] Listo para producci√≥n

## üéâ ¬°Listo!

El coordinador de scraping est√° completamente implementado, testeado y documentado. 
¬°Listo para usar en producci√≥n!

---

**Nota**: Este coordinador es parte del Ticket 8 del proyecto de agregador de noticias argentinas.
